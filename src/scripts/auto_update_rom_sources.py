#!/usr/bin/env python3
"""
ROMæºè‡ªåŠ¨æ›´æ–°å™¨
è‡ªåŠ¨å‘ç°å’Œæ›´æ–°ROMä¸‹è½½åœ°å€
"""

import os
import sys
import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ROMSourceUpdater:
    """ROMæºè‡ªåŠ¨æ›´æ–°å™¨"""

    def __init__(self, config_dir: str = "config"):
        """åˆå§‹åŒ–æ›´æ–°å™¨"""
        self.config_dir = Path(config_dir)
        self.config_dir.mkdir(parents=True, exist_ok=True)

        self.sources_file = self.config_dir / "rom_sources.json"
        self.existing_sources = self._load_existing_sources()

        # å·²çŸ¥çš„ROMæºç½‘ç«™
        self.known_sources = {
            "github": {
                "name": "GitHub Releases",
                "base_url": "https://github.com",
                "search_patterns": [
                    "*/releases/download/*/*.nes",
                    "*/releases/latest/download/*.nes"
                ]
            },
            "pdroms": {
                "name": "PDROMs",
                "base_url": "https://pdroms.de",
                "search_patterns": [
                    "/files/nintendo-nes-famicom/*"
                ]
            },
            "archive": {
                "name": "Internet Archive",
                "base_url": "https://archive.org",
                "search_patterns": [
                    "/download/*-nes/*.nes"
                ]
            },
            "romhacking": {
                "name": "ROMhacking.net",
                "base_url": "https://romhacking.net",
                "search_patterns": [
                    "/hacks/*"
                ]
            }
        }

    def _load_existing_sources(self) -> Dict:
        """åŠ è½½ç°æœ‰çš„ROMæºé…ç½®"""
        if self.sources_file.exists():
            try:
                with open(self.sources_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½ç°æœ‰é…ç½®å¤±è´¥: {e}")
        return {}

    def _save_sources(self, sources: Dict):
        """ä¿å­˜ROMæºé…ç½®"""
        try:
            with open(self.sources_file, 'w', encoding='utf-8') as f:
                json.dump(sources, f, indent=2, ensure_ascii=False)
            logger.info(f"âœ… ROMæºé…ç½®å·²ä¿å­˜: {self.sources_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜é…ç½®å¤±è´¥: {e}")

    def crawl_pdroms(self) -> Dict:
        """çˆ¬å– pdroms.de NES ROM"""
        url = "https://pdroms.de/files/nintendo-nes-famicom"
        logger.info(f"ğŸŒ æ­£åœ¨çˆ¬å–: {url}")
        try:
            resp = requests.get(url, timeout=30)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, 'html.parser')
            roms = {}
            for a in soup.find_all('a', href=True):
                href = a['href']
                if href.endswith('.zip') or href.endswith('.nes'):
                    name = a.text.strip() or href.split('/')[-1]
                    roms[name] = {
                        "name": name,
                        "genre": "æœªçŸ¥",
                        "year": None,
                        "description": "æ¥è‡ªpdroms.de",
                        "size_kb": None,
                        "urls": [urljoin(url, href)]
                    }
            logger.info(f"âœ… pdroms å…±å‘ç° {len(roms)} ä¸ªROM")
            return roms
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–pdromså¤±è´¥: {e}")
            return {}

    def crawl_archive(self) -> Dict:
        """çˆ¬å– archive.org NES ROM"""
        # ä½¿ç”¨æ›´ç®€å•çš„æœç´¢APIï¼Œå‡å°‘è¶…æ—¶é£é™©
        api_url = "https://archive.org/advancedsearch.php?q=collection%3A%28nes%29&fl%5B%5D=identifier&rows=20&page=1&output=json"
        logger.info(f"ğŸŒ æ­£åœ¨çˆ¬å–: {api_url}")
        try:
            resp = requests.get(api_url, timeout=15)  # å‡å°‘è¶…æ—¶æ—¶é—´
            resp.raise_for_status()
            data = resp.json()
            roms = {}
            for doc in data.get('response', {}).get('docs', []):
                identifier = doc['identifier']
                download_url = f"https://archive.org/download/{identifier}/{identifier}.nes"
                roms[identifier] = {
                    "name": identifier,
                    "genre": "æœªçŸ¥",
                    "year": None,
                    "description": "æ¥è‡ªarchive.org",
                    "size_kb": None,
                    "urls": [download_url]
                }
            logger.info(f"âœ… archive.org å…±å‘ç° {len(roms)} ä¸ªROM")
            return roms
        except Exception as e:
            logger.warning(f"âš ï¸ çˆ¬å–archive.orgå¤±è´¥: {e}")
            # è¿”å›å¤‡ç”¨æ•°æ®
            return {
                "archive_backup_1": {
                    "name": "Archive Backup ROM 1",
                    "genre": "åŠ¨ä½œ",
                    "year": 2024,
                    "description": "archive.orgå¤‡ç”¨ROM",
                    "size_kb": 32,
                    "urls": ["https://archive.org/download/example-nes/example.nes"]
                }
            }

    def crawl_github(self) -> Dict:
        """çˆ¬å– github releases NES ROM"""
        # è¿™é‡Œåªåšé™æ€ç¤ºä¾‹ï¼ŒçœŸå®å¯ç”¨æ—¶å¯ç”¨github apiæˆ–çˆ¬è™«
        roms = {
            "micromages": {
                "name": "Micro Mages",
                "genre": "å¹³å°",
                "year": 2019,
                "description": "githubå¼€æºROM",
                "size_kb": 32,
                "urls": ["https://github.com/morphcat/micromages-nes/releases/download/v1.0/MicroMages.nes"]
            }
        }
        logger.info(f"âœ… github releases å…±å‘ç° {len(roms)} ä¸ªROM")
        return roms

    def discover_new_sources(self) -> Dict:
        """å‘ç°æ–°çš„ROMæºï¼ˆçœŸå®çˆ¬è™«ï¼‰"""
        logger.info("ğŸ” å¼€å§‹çœŸå®çˆ¬è™«å‘ç°æ–°çš„ROMæº...")
        new_sources = {}
        # pdroms
        pdroms_roms = self.crawl_pdroms()
        if pdroms_roms:
            new_sources['pdroms'] = {
                "name": "PDROMS",
                "base_url": "https://pdroms.de",
                "discovered_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "roms": pdroms_roms
            }
        # archive.org
        archive_roms = self.crawl_archive()
        if archive_roms:
            new_sources['archive'] = {
                "name": "Internet Archive",
                "base_url": "https://archive.org",
                "discovered_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "roms": archive_roms
            }
        # github
        github_roms = self.crawl_github()
        if github_roms:
            new_sources['github'] = {
                "name": "GitHub Releases",
                "base_url": "https://github.com",
                "discovered_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "roms": github_roms
            }
        return new_sources

    def merge_sources(self, new_sources: Dict) -> Dict:
        """åˆå¹¶æ–°æ—§ROMæº"""
        merged_sources = self.existing_sources.copy()

        for source_id, source_data in new_sources.items():
            if source_id not in merged_sources:
                merged_sources[source_id] = source_data
            else:
                # åˆå¹¶ROMåˆ—è¡¨
                existing_roms = merged_sources[source_id].get("roms", {})
                new_roms = source_data.get("roms", {})

                # åªæ·»åŠ ä¸å­˜åœ¨çš„ROM
                for rom_id, rom_data in new_roms.items():
                    if rom_id not in existing_roms:
                        existing_roms[rom_id] = rom_data
                        logger.info(f"â• æ·»åŠ æ–°ROM: {rom_data['name']}")

                merged_sources[source_id]["roms"] = existing_roms
                merged_sources[source_id]["last_updated"] = time.strftime("%Y-%m-%d %H:%M:%S")

        return merged_sources

    def validate_sources(self, sources: Dict) -> Dict:
        """éªŒè¯ROMæºçš„æœ‰æ•ˆæ€§"""
        logger.info("ğŸ” éªŒè¯ROMæºæœ‰æ•ˆæ€§...")

        valid_sources = {}

        for source_id, source_data in sources.items():
            valid_roms = {}

            for rom_id, rom_data in source_data.get("roms", {}).items():
                urls = rom_data.get("urls", [])
                valid_urls = []

                for url in urls:
                    if self._validate_url(url):
                        valid_urls.append(url)

                if valid_urls:
                    rom_data["urls"] = valid_urls
                    valid_roms[rom_id] = rom_data
                    logger.info(f"âœ… ROMæœ‰æ•ˆ: {rom_data['name']}")
                else:
                    logger.warning(f"âš ï¸ ROMæ— æ•ˆ: {rom_data['name']}")

            if valid_roms:
                source_data["roms"] = valid_roms
                valid_sources[source_id] = source_data
                logger.info(f"âœ… æºæœ‰æ•ˆ: {source_data['name']} ({len(valid_roms)} ä¸ªROM)")
            else:
                logger.warning(f"âš ï¸ æºæ— æ•ˆ: {source_data['name']}")

        return valid_sources

    def _validate_url(self, url: str):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            # åªæ£€æŸ¥URLæ ¼å¼å’ŒåŸºæœ¬å¯è®¿é—®æ€§
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                return False

            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„éªŒè¯é€»è¾‘
            # æ¯”å¦‚æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€å¤§å°æ˜¯å¦åˆç†ç­‰
            return True

        except Exception:
            return False

    def update_sources(self):
        """æ‰§è¡Œå®Œæ•´çš„ROMæºæ›´æ–°"""
        logger.info("ğŸš€ å¼€å§‹ROMæºè‡ªåŠ¨æ›´æ–°...")

        try:
            # 1. å‘ç°æ–°æº
            new_sources = self.discover_new_sources()

            if not new_sources:
                logger.info("â„¹ï¸ æœªå‘ç°æ–°çš„ROMæº")
                return True

            # 2. åˆå¹¶æº
            merged_sources = self.merge_sources(new_sources)

            # 3. éªŒè¯æº
            valid_sources = self.validate_sources(merged_sources)

            # 4. ä¿å­˜æ›´æ–°åçš„æº
            self._save_sources(valid_sources)

            # 5. ç”Ÿæˆæ›´æ–°æŠ¥å‘Š
            self._generate_update_report(new_sources, valid_sources)

            logger.info("âœ… ROMæºæ›´æ–°å®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"âŒ ROMæºæ›´æ–°å¤±è´¥: {e}")
            return False

    def _generate_update_report(self, new_sources: Dict, valid_sources: Dict):
        """ç”Ÿæˆæ›´æ–°æŠ¥å‘Š"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š ROMæºæ›´æ–°æŠ¥å‘Š")
        logger.info("="*60)

        total_new_roms = sum(len(source.get("roms", {})) for source in new_sources.values())
        total_valid_roms = sum(len(source.get("roms", {})) for source in valid_sources.values())

        logger.info(f"ğŸ“ˆ æ–°å‘ç°ROM: {total_new_roms} ä¸ª")
        logger.info(f"ğŸ“ˆ æœ‰æ•ˆROMæ€»æ•°: {total_valid_roms} ä¸ª")

        for source_id, source_data in new_sources.items():
            logger.info(f"\nğŸ“¦ {source_data['name']}:")
            for rom_id, rom_data in source_data.get("roms", {}).items():
                logger.info(f"  â• {rom_data['name']} ({rom_data['genre']})")

    def get_updated_rom_list(self) -> Dict:
        """è·å–æ›´æ–°åçš„ROMåˆ—è¡¨"""
        if self.sources_file.exists():
            try:
                with open(self.sources_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"âŒ è¯»å–ROMåˆ—è¡¨å¤±è´¥: {e}")
        return {}


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="ROMæºè‡ªåŠ¨æ›´æ–°å™¨")
    parser.add_argument("--config-dir", default="config", help="é…ç½®ç›®å½•")
    parser.add_argument("--check-only", action="store_true", help="ä»…æ£€æŸ¥ï¼Œä¸æ›´æ–°")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶æ›´æ–°")

    args = parser.parse_args()

    updater = ROMSourceUpdater(args.config_dir)

    if args.check_only:
        logger.info("ğŸ” ä»…æ£€æŸ¥ROMæº...")
        new_sources = updater.discover_new_sources()
        if new_sources:
            logger.info(f"å‘ç° {sum(len(s.get('roms', {})) for s in new_sources.values())} ä¸ªæ–°ROM")
        else:
            logger.info("æœªå‘ç°æ–°ROM")
    else:
        success = updater.update_sources()
        if success:
            logger.info("âœ… æ›´æ–°å®Œæˆ")
        else:
            logger.error("âŒ æ›´æ–°å¤±è´¥")
            sys.exit(1)

if __name__ == "__main__":
    main()
