#!/usr/bin/env python3
"""
é¡¹ç›®æ¸…ç†å·¥å…·
æ¸…ç†é‡å¤æ–‡ä»¶ã€æ— ç”¨æ–‡ä»¶ã€ä¼˜åŒ–é¡¹ç›®ç»“æ„
"""

import os
import shutil
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict

class ProjectCleaner:
    """é¡¹ç›®æ¸…ç†å™¨"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root).resolve()
        self.duplicate_files = []
        self.empty_files = []
        self.large_files = []
        self.unused_files = []
        
    def find_duplicate_files(self) -> List[Tuple[str, List[str]]]:
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶"""
        print("ğŸ” æŸ¥æ‰¾é‡å¤æ–‡ä»¶...")
        
        file_hashes = defaultdict(list)
        
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file() and not self._should_skip_file(file_path):
                try:
                    file_hash = self._calculate_file_hash(file_path)
                    file_hashes[file_hash].append(str(file_path))
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è®¡ç®—æ–‡ä»¶å“ˆå¸Œ {file_path}: {e}")
        
        # æ‰¾å‡ºé‡å¤æ–‡ä»¶
        duplicates = []
        for file_hash, file_list in file_hashes.items():
            if len(file_list) > 1:
                duplicates.append((file_hash, file_list))
        
        self.duplicate_files = duplicates
        print(f"ğŸ“„ æ‰¾åˆ° {len(duplicates)} ç»„é‡å¤æ–‡ä»¶")
        
        return duplicates
    
    def find_empty_files(self) -> List[str]:
        """æŸ¥æ‰¾ç©ºæ–‡ä»¶"""
        print("ğŸ” æŸ¥æ‰¾ç©ºæ–‡ä»¶...")
        
        empty_files = []
        
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file() and not self._should_skip_file(file_path):
                try:
                    if file_path.stat().st_size == 0:
                        empty_files.append(str(file_path))
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•æ£€æŸ¥æ–‡ä»¶å¤§å° {file_path}: {e}")
        
        self.empty_files = empty_files
        print(f"ğŸ“„ æ‰¾åˆ° {len(empty_files)} ä¸ªç©ºæ–‡ä»¶")
        
        return empty_files
    
    def find_large_files(self, size_limit_mb: int = 10) -> List[Tuple[str, int]]:
        """æŸ¥æ‰¾å¤§æ–‡ä»¶"""
        print(f"ğŸ” æŸ¥æ‰¾å¤§äº {size_limit_mb}MB çš„æ–‡ä»¶...")
        
        large_files = []
        size_limit_bytes = size_limit_mb * 1024 * 1024
        
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file() and not self._should_skip_file(file_path):
                try:
                    file_size = file_path.stat().st_size
                    if file_size > size_limit_bytes:
                        large_files.append((str(file_path), file_size))
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•æ£€æŸ¥æ–‡ä»¶å¤§å° {file_path}: {e}")
        
        # æŒ‰å¤§å°æ’åº
        large_files.sort(key=lambda x: x[1], reverse=True)
        
        self.large_files = large_files
        print(f"ğŸ“„ æ‰¾åˆ° {len(large_files)} ä¸ªå¤§æ–‡ä»¶")
        
        return large_files
    
    def find_unused_files(self) -> List[str]:
        """æŸ¥æ‰¾å¯èƒ½æœªä½¿ç”¨çš„æ–‡ä»¶"""
        print("ğŸ” æŸ¥æ‰¾å¯èƒ½æœªä½¿ç”¨çš„æ–‡ä»¶...")
        
        unused_patterns = [
            "*.tmp", "*.temp", "*.bak", "*.backup", "*.old",
            "*.log", "*.cache", "*.pyc", "*.pyo", "*.pyd",
            ".DS_Store", "Thumbs.db", "desktop.ini"
        ]
        
        unused_files = []
        
        for pattern in unused_patterns:
            for file_path in self.project_root.rglob(pattern):
                if file_path.is_file():
                    unused_files.append(str(file_path))
        
        self.unused_files = unused_files
        print(f"ğŸ“„ æ‰¾åˆ° {len(unused_files)} ä¸ªå¯èƒ½æœªä½¿ç”¨çš„æ–‡ä»¶")
        
        return unused_files
    
    def clean_empty_directories(self) -> List[str]:
        """æ¸…ç†ç©ºç›®å½•"""
        print("ğŸ” æ¸…ç†ç©ºç›®å½•...")
        
        removed_dirs = []
        
        # å¤šæ¬¡éå†ï¼Œå› ä¸ºåˆ é™¤å­ç›®å½•å¯èƒ½ä½¿çˆ¶ç›®å½•å˜ç©º
        for _ in range(5):  # æœ€å¤š5æ¬¡è¿­ä»£
            empty_dirs = []
            
            for dir_path in self.project_root.rglob("*"):
                if dir_path.is_dir() and not self._should_skip_directory(dir_path):
                    try:
                        # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©º
                        if not any(dir_path.iterdir()):
                            empty_dirs.append(dir_path)
                    except Exception as e:
                        print(f"âš ï¸ æ— æ³•æ£€æŸ¥ç›®å½• {dir_path}: {e}")
            
            if not empty_dirs:
                break
            
            # åˆ é™¤ç©ºç›®å½•
            for dir_path in empty_dirs:
                try:
                    dir_path.rmdir()
                    removed_dirs.append(str(dir_path))
                    print(f"ğŸ—‘ï¸ åˆ é™¤ç©ºç›®å½•: {dir_path}")
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•åˆ é™¤ç›®å½• {dir_path}: {e}")
        
        print(f"ğŸ—‘ï¸ åˆ é™¤äº† {len(removed_dirs)} ä¸ªç©ºç›®å½•")
        return removed_dirs
    
    def remove_duplicate_files(self, keep_first: bool = True) -> List[str]:
        """åˆ é™¤é‡å¤æ–‡ä»¶"""
        print("ğŸ—‘ï¸ åˆ é™¤é‡å¤æ–‡ä»¶...")
        
        removed_files = []
        
        for file_hash, file_list in self.duplicate_files:
            if len(file_list) > 1:
                # ä¿ç•™ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œåˆ é™¤å…¶ä»–çš„
                files_to_remove = file_list[1:] if keep_first else file_list[:-1]
                
                for file_path in files_to_remove:
                    try:
                        Path(file_path).unlink()
                        removed_files.append(file_path)
                        print(f"ğŸ—‘ï¸ åˆ é™¤é‡å¤æ–‡ä»¶: {file_path}")
                    except Exception as e:
                        print(f"âš ï¸ æ— æ³•åˆ é™¤æ–‡ä»¶ {file_path}: {e}")
        
        print(f"ğŸ—‘ï¸ åˆ é™¤äº† {len(removed_files)} ä¸ªé‡å¤æ–‡ä»¶")
        return removed_files
    
    def remove_unused_files(self) -> List[str]:
        """åˆ é™¤æœªä½¿ç”¨çš„æ–‡ä»¶"""
        print("ğŸ—‘ï¸ åˆ é™¤æœªä½¿ç”¨çš„æ–‡ä»¶...")
        
        removed_files = []
        
        for file_path in self.unused_files:
            try:
                Path(file_path).unlink()
                removed_files.append(file_path)
                print(f"ğŸ—‘ï¸ åˆ é™¤æœªä½¿ç”¨æ–‡ä»¶: {file_path}")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•åˆ é™¤æ–‡ä»¶ {file_path}: {e}")
        
        print(f"ğŸ—‘ï¸ åˆ é™¤äº† {len(removed_files)} ä¸ªæœªä½¿ç”¨æ–‡ä»¶")
        return removed_files
    
    def optimize_project_structure(self) -> Dict[str, List[str]]:
        """ä¼˜åŒ–é¡¹ç›®ç»“æ„"""
        print("ğŸ”§ ä¼˜åŒ–é¡¹ç›®ç»“æ„...")
        
        optimizations = {
            "moved_files": [],
            "created_directories": [],
            "updated_imports": []
        }
        
        # ç¡®ä¿æ ‡å‡†ç›®å½•å­˜åœ¨
        standard_dirs = [
            "src", "tests", "docs", "config", "data", "build", "tools"
        ]
        
        for dir_name in standard_dirs:
            dir_path = self.project_root / dir_name
            if not dir_path.exists():
                dir_path.mkdir(parents=True, exist_ok=True)
                optimizations["created_directories"].append(str(dir_path))
                print(f"ğŸ“ åˆ›å»ºç›®å½•: {dir_path}")
        
        return optimizations
    
    def generate_cleanup_report(self) -> Dict:
        """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
        return {
            "duplicate_files": len(self.duplicate_files),
            "empty_files": len(self.empty_files),
            "large_files": len(self.large_files),
            "unused_files": len(self.unused_files),
            "duplicate_file_details": [
                {
                    "hash": file_hash,
                    "files": file_list,
                    "count": len(file_list)
                }
                for file_hash, file_list in self.duplicate_files
            ],
            "large_file_details": [
                {
                    "file": file_path,
                    "size_mb": round(size / (1024 * 1024), 2)
                }
                for file_path, size in self.large_files
            ]
        }
    
    def run_full_cleanup(self, remove_duplicates: bool = True, 
                        remove_unused: bool = True) -> Dict:
        """è¿è¡Œå®Œæ•´æ¸…ç†"""
        print("ğŸš€ å¼€å§‹é¡¹ç›®æ¸…ç†...")
        
        # åˆ†æé˜¶æ®µ
        self.find_duplicate_files()
        self.find_empty_files()
        self.find_large_files()
        self.find_unused_files()
        
        # æ¸…ç†é˜¶æ®µ
        removed_files = []
        removed_dirs = []
        
        if remove_duplicates:
            removed_files.extend(self.remove_duplicate_files())
        
        if remove_unused:
            removed_files.extend(self.remove_unused_files())
        
        removed_dirs.extend(self.clean_empty_directories())
        
        # ä¼˜åŒ–é˜¶æ®µ
        optimizations = self.optimize_project_structure()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_cleanup_report()
        report.update({
            "removed_files": len(removed_files),
            "removed_directories": len(removed_dirs),
            "optimizations": optimizations
        })
        
        print("âœ… é¡¹ç›®æ¸…ç†å®Œæˆ!")
        print(f"ğŸ“Š æ¸…ç†ç»Ÿè®¡:")
        print(f"  - åˆ é™¤æ–‡ä»¶: {len(removed_files)}")
        print(f"  - åˆ é™¤ç›®å½•: {len(removed_dirs)}")
        print(f"  - é‡å¤æ–‡ä»¶ç»„: {len(self.duplicate_files)}")
        print(f"  - å¤§æ–‡ä»¶: {len(self.large_files)}")
        
        return report
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def _should_skip_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æ–‡ä»¶"""
        skip_patterns = [
            ".git", "__pycache__", ".pytest_cache", "node_modules",
            ".venv", "venv", "env", ".tox", "build", "dist"
        ]
        
        return any(pattern in str(file_path) for pattern in skip_patterns)
    
    def _should_skip_directory(self, dir_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡ç›®å½•"""
        skip_dirs = {
            ".git", "__pycache__", ".pytest_cache", "node_modules",
            ".venv", "venv", "env", ".tox"
        }
        
        return dir_path.name in skip_dirs

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    import json
    
    parser = argparse.ArgumentParser(description="é¡¹ç›®æ¸…ç†å·¥å…·")
    parser.add_argument("--project-root", default=".", help="é¡¹ç›®æ ¹ç›®å½•")
    parser.add_argument("--output", default="docs/reports/cleanup_report.json", help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶")
    parser.add_argument("--no-remove-duplicates", action="store_true", help="ä¸åˆ é™¤é‡å¤æ–‡ä»¶")
    parser.add_argument("--no-remove-unused", action="store_true", help="ä¸åˆ é™¤æœªä½¿ç”¨æ–‡ä»¶")
    parser.add_argument("--dry-run", action="store_true", help="åªåˆ†æä¸åˆ é™¤")
    
    args = parser.parse_args()
    
    cleaner = ProjectCleaner(args.project_root)
    
    if args.dry_run:
        print("ğŸ” è¿è¡Œåˆ†ææ¨¡å¼ï¼ˆä¸åˆ é™¤æ–‡ä»¶ï¼‰...")
        cleaner.find_duplicate_files()
        cleaner.find_empty_files()
        cleaner.find_large_files()
        cleaner.find_unused_files()
        report = cleaner.generate_cleanup_report()
    else:
        report = cleaner.run_full_cleanup(
            remove_duplicates=not args.no_remove_duplicates,
            remove_unused=not args.no_remove_unused
        )
    
    # ä¿å­˜æŠ¥å‘Š
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ æ¸…ç†æŠ¥å‘Šå·²ä¿å­˜: {args.output}")

if __name__ == "__main__":
    main()
